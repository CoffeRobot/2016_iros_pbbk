%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10pt, conference]{ieeeconf}                             
\IEEEoverridecommandlockouts         % Needed if you want to use the \thanks command
                                                     
\overrideIEEEmargins                                                            


\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath}
\usepackage{amssymb,longtable,calc}
\usepackage{mathptmx}
\usepackage[T1]{fontenc}                                                        
\usepackage[utf8]{inputenc}                                                     
\usepackage[english]{babel}                                                     
\usepackage{epsfig}                                                             
\usepackage{subfigure}                                                          
\usepackage{textcomp} %<- allows to use \textdegree but may overwrite           
                      %other settings                                           
\usepackage[textwidth=2cm,colorinlistoftodos]{todonotes} %add disable   
                                %to not show the todos                          
\usepackage{tikz}                                                               
\usetikzlibrary{arrows,positioning,fit,shapes,calc}
\usetikzlibrary{matrix}
\usepackage{flushend}                                                           
\usepackage{hyperref}  
\usepackage{amsmath}    

\usepackage[utf8]{inputenc}   
\usepackage[]{algorithm2e}
\usepackage{amsmath}


\usepackage{pgfplots} 
\usepackage{pgfplotstable}

\usepackage{cite}

% helper packages to work on the draft
\usepackage[tikz]{bclogo}
\usepackage{lipsum}

\usepackage{standalone}

\pgfplotsset{compat=newest}
\pgfplotsset{ 
  tick label style={font=\footnotesize}, 
  label style={font=\footnotesize}, 
  legend style={font=\footnotesize},
  title style = {font=\small}
}
\pgfplotscreateplotcyclelist{line style}{% 
  solid, mark options = {scale = .75}, every mark/.append style={fill=gray},mark=*\\% 
  densely dashed,mark options = {scale = .75},every mark/.append style={solid,fill=gray},mark=*\\% 
  densely dotted,mark options = {scale = .75},every mark/.append style={solid,fill=gray},mark=*\\% 
  dashed,mark options = {scale = .75},every mark/.append style={solid,fill=gray},mark=*\\% 
  dotted,mark options = {scale = .75},every mark/.append style={solid,fill=gray},mark=*\\% 
}
\pgfplotscreateplotcyclelist{bar style}{% 
  solid, fill=black!60!white\\%
  solid, fill=black!45!white\\%
  solid, fill=black!35!white\\%
  solid, fill=black!25!white\\%
}


\usepackage{xspace}
\makeatletter                                                                   
\DeclareTextCommandDefault{\textregisteredalt}{\footnotesize\textcircled{%      
      \check@mathfonts\fontsize\sf@size\z@\math@fontsfalse\selectfont R}}       
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}                     
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}                             
\def\eg{e.g\onedot}                                                             
\def\ie{i.e\onedot}                                                             
\def\vgl{see }                                                                  
\def\Fig{Fig\onedot }                                                           
\def\Tab{Tab\onedot }                                                           
\def\Eq{Eq\onedot }
\def\Sec{Sec\onedot}                                                            
\def\etc{etc\onedot}                                                            
\def\etal{\textsl{et al}\onedot}                                                
\def\argmin{\mathop{\rm arg\,min}}                                              
\makeatother
                                                                    
\definecolor{lightGray}{rgb}{0.0,0.0,0.0}
\definecolor{kthColor}{RGB}{26,84,166}
                                                                                
\title{\LARGE \bf Feature Descriptors for Tracking by Detection: a Benchmark}                                                                               
                                                                                
                                                                                
\author{Alessandro Pieropan ~~~~ MÃ¥rten Bj{\"o}rkman  ~~~~ Niklas Bergstr{\"o}m ~~~~ Danica Kragic%
\thanks{This research has been supported by he Japan Society for the Promotion of Science (JSPS)}
\thanks{The GPU used for this research was donated by the NVIDIA Corporation.}
\thanks{MB and DN are with CVAP/CAS, KTH, Stockholm, Sweden, {\tt celle,dani@kth.se}. AP and NB are with the University of Tokyo, Japan, {\tt alessandro\_pieropan}, {\tt niklas\_bergstrom@ipc.i.u$-$tokyo.ac.jp}.}}

\begin{document}                                                                
                                                                                
\maketitle                                                                      
\thispagestyle{empty}                                                           
\pagestyle{empty}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ABSTRACT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
In this paper, we provide an extensive evaluation of the performance of local descriptors for tracking applications.
Many different descriptors have been proposed in the literature for a wide range of application in Computer Vision such as object recognition and 3D reconstruction. More recently, due to fast key-point detectors, feature descriptor can be used in online tracking frameworks. However, while much effort has been spent on evaluating their performance in terms of distinctiveness and robustness to image transformations, very little has been done in the contest of tracking. Our evaluation is performed in terms of distinctiveness, tracking precision and tracking speed. Our results show that binary descriptors like ORB or BRISK have comparable results to SIFT or AKAZE due to a higher number of key-points.    

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% INTRODUCTION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{}
\section{INTRODUCTION}
\label{sec:introduction}



Local regions of interest or key-point descriptors are widely used in Computer Vision for application such as object recognition and retrieval, 3D reconstruction and motion tracking. SIFT \cite{lowe04} is widely considered as one of the most robust feature descriptors, providing distinctiveness and invariance to common image transformations such as rotation and scale. However such a robustness comes with a computational cost. 

Recently there is much concern about efficiency caused mainly by two important factors. First there is a stable growth of portable camera enabled devices with limited computing power. Second, computer vision databases are steadily increasing in size. As a consequence, there is a growing interest within the computer vision community on fast key-point detectors and binary descriptors that can dramatically decrease the computational cost of detecting and matching local regions of interest. BRIEF \cite{calonder10} feature descriptor in combination with FAST \cite{rosten06} key-point detector is among the first attempts in this direction making it suitable for real time applications. However, despite the improvement in performance, BRIEF descriptor is not very robust to image transformations. This underlines the difficulty in finding a good compromise between two competing characteristics: distinctiveness and fast computation. 

\begin{figure}[t]
	\vspace{2mm}
\centerline{%
	\subfigure{\includegraphics[width=0.48\linewidth]{imgs/intro/intro1.png}}
	%\centerline{%
	\subfigure{\includegraphics[width=0.48\linewidth]{imgs/intro/intro3.png}}}
	\vspace{-2mm}
\centerline{%
	\subfigure{\includegraphics[width=0.48\linewidth]{imgs/intro/intro2.png}}
	\subfigure{\includegraphics[width=0.48\linewidth]{imgs/intro/intro4.png}}}
\caption{Examples showing some of the videos the descriptors has been tested on and the tracking results expressed as a coloured bounding box. Clearly some feature descriptors show more precise tracking.}
\vspace{-3mm}
\label{fig:intro}
\end{figure}

More attempts has been done in this direction. BRISK \cite{leutenegger11} and ORB \cite{rublee11} propose some modification of the BRIEF and FAST in order to achieve scale and rotation invariance. The mentioned descriptors are faster that SIFT and have comparable matching precision with small image transformation. More recently the binary feature descriptor KAZE \cite{alcantarilla12} presented comparable results to SIFT on a standard dataset \cite{mikolajczyk05} designed to evaluate the robustness of local descriptors on several image transformations. Moreover, by using fast non linear filtering techniques Fast Explicit Diffusion \cite{goesele2010}, its accelerated version AKAZE \cite{alcantarilla13} could also beat SIFT in computational cost.\\
The main reason why AKAZE could outperform SIFT relies in the use of non linear filtering techniques. However, given the increasing use of GPUs in computer vision, it is not clear if such techniques can easily parallelizable on a GPU architecture as opposed to Gaussian filtering employed in SIFT. A recent work \cite{jiang2015} has shown that it is possible to deploy AKAZE descriptor on a specialized hardware and achieve a good speedup compared to the original implementation. By implementing the descriptor in CUDA we intend to analyze its performance using a GPU and compare it against the GPU implementations of their counterpart descriptors.

%\missingfigure[figwidth=0.98\linewidth]{Intro figure showing tracker benchmark}

These extremely fast local descriptors not only improve the performance of vision tasks such as object retrieval and 3D reconstruction but they can be used in real-time tracking system. Yet, to the best of our knowledge, very little work has been done in evaluating key-points descriptor for the specific purpose of tracking. This work aims to target that issue by providing a fair and comprehensive evaluation of the most well known descriptors. We use the recall-precision measure to evaluate the matching distinctiveness of the features and we calculate the tracking precision by integrating each local descriptor in a key-point based tracker \cite{pieropan15}. Since there is no dataset designed to test local descriptors for tracking purposes , the videos used in the evaluation are gathered from well known tracking datasets described in \cite{wu2013,nebehay2014,hare2011}. We want to provide a practical guideline to binary descriptors for the specific task of tracking since it is not clear yet that a descriptor designed for image recognition is well suited for tracking.
 
The contribution of this work is four-fold:\\
\textbf{Dataset.} We collected 47 sequences from different public available datasets and annotated the ground-truth in a standardized format to ease the evaluation.\\
\textbf{Cuda Akaze.} Given the growing interest in the AKAZE feature descriptor and the lack of a GPU implementation, we provide our own using CUDA.\\
\textbf{Evaluation Library.} We integrated the most well known descriptors in a state-of-the-art key-point based 2D tracker and we provide an interface to enable the integration of new feature descriptors.\\
\textbf{Evaluation.}  There are three criteria the local descriptors are tested upon: distinctiveness, tracking precision and speed. First we measure the distinctiveness by matching the feature descriptors extracted in the first frame of the sequence and by computing the recall-precision. Then we calculate the tracking by integrating each feature descriptor in a key-point based tracker and we calculate the well known overlap accuracy for low, medium and high precision requirements. Last we profile the performance of each descriptor.

The dataset, our AKAZE implementation and all the code to perform the benchmark will be publicly available.



%% %%%%%%%%%%%%%%%%%%%%%%%%%%% RELATED WORK %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\input{related}

%% %%%%%%%%%%%%%%%%%%%%%%%%%%% DATASET AND EVALUATION DESCRIPTION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\input{dataset}

%% %%%%%%%%%%%%%%%%%%%%%%%%%%% BENCHMARK %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{benchmark}

%% %%%%%%%%%%%%%%%%%%%%%%%%%%% BENCHMARK %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{results}

%% %%%%%%%%%%%%%%%%%%%%%%%%%%% CONCLUSION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{conclusion}

\bibliographystyle{unsrt}
\bibliography{ref}

%\input{additional}

\end{document}
