%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10pt, conference]{ieeeconf}                             
\IEEEoverridecommandlockouts         % Needed if you want to use the \thanks command
                                                     
\overrideIEEEmargins                                                            


\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath}
\usepackage{amssymb,longtable,calc}
\usepackage{mathptmx}
\usepackage[T1]{fontenc}                                                        
\usepackage[utf8]{inputenc}                                                     
\usepackage[english]{babel}                                                     
\usepackage{epsfig}                                                             
\usepackage{subfigure}                                                          
\usepackage{textcomp} %<- allows to use \textdegree but may overwrite           
                      %other settings                                           
\usepackage[textwidth=2cm,colorinlistoftodos,disable]{todonotes} %add disable   
                                %to not show the todos                          
\usepackage{tikz}                                                               
\usetikzlibrary{arrows,positioning,fit,shapes,calc}
\usetikzlibrary{matrix}
\usepackage{flushend}                                                           
\usepackage{hyperref}  
\usepackage{amsmath}                                                         
% \usepackage{multirow}   

\usepackage{algorithm}    
\usepackage{algorithmic}

\usepackage{pgfplots} 
\usepackage{pgfplotstable}

\usepackage{cite}

% helper packages to work on the draft
\usepackage[tikz]{bclogo}
\usepackage{lipsum}

\usepackage{standalone}

\pgfplotsset{compat=newest}
\pgfplotsset{ 
  tick label style={font=\footnotesize}, 
  label style={font=\footnotesize}, 
  legend style={font=\footnotesize},
  title style = {font=\small}
}
\pgfplotscreateplotcyclelist{line style}{% 
  solid, mark options = {scale = .75}, every mark/.append style={fill=gray},mark=*\\% 
  densely dashed,mark options = {scale = .75},every mark/.append style={solid,fill=gray},mark=*\\% 
  densely dotted,mark options = {scale = .75},every mark/.append style={solid,fill=gray},mark=*\\% 
  dashed,mark options = {scale = .75},every mark/.append style={solid,fill=gray},mark=*\\% 
  dotted,mark options = {scale = .75},every mark/.append style={solid,fill=gray},mark=*\\% 
}
\pgfplotscreateplotcyclelist{bar style}{% 
  solid, fill=black!60!white\\%
  solid, fill=black!45!white\\%
  solid, fill=black!35!white\\%
  solid, fill=black!25!white\\%
}


\usepackage{xspace}
\makeatletter                                                                   
\DeclareTextCommandDefault{\textregisteredalt}{\footnotesize\textcircled{%      
      \check@mathfonts\fontsize\sf@size\z@\math@fontsfalse\selectfont R}}       
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}                     
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}                             
\def\eg{e.g\onedot}                                                             
\def\ie{i.e\onedot}                                                             
\def\vgl{see }                                                                  
\def\Fig{Fig\onedot }                                                           
\def\Tab{Tab\onedot }                                                           
\def\Eq{Eq\onedot }
\def\Sec{Sec\onedot}                                                            
\def\etc{etc\onedot}                                                            
\def\etal{\textsl{et al}\onedot}                                                
\def\argmin{\mathop{\rm arg\,min}}                                              
\makeatother
                                                                    
\definecolor{lightGray}{rgb}{0.0,0.0,0.0}
                                                                                
\title{\LARGE \bf Feature Descriptors for Tracking by Detection: a Benchmark}                                                                               
                                                                                
                                                                                
\author{Alessandro Pieropan ~~~~ MÃ¥rten Bj{\"o}rkman  ~~~~ Niklas Bergstr{\"o}m ~~~~  Masatoshi Ishikawa ~~~~ Danica Kragic%
\thanks{The GPU used for this research was donated by the NVIDIA Corporation.}
\thanks{MB and DN are with CVAP/CAS, KTH, Stockholm, Sweden, {\tt celle,dani@kth.se}. AP, NB and MI are with the University of Tokyo, Japan, {\tt }.}}

\begin{document}                                                                
                                                                                
\maketitle                                                                      
\thispagestyle{empty}                                                           
\pagestyle{empty}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ABSTRACT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

Humans are able to merge information from multiple perceptional modalities and formulate a 
coherent representation of the world. Our thesis is that robots need to do the same in order to operate robustly and autonomously in an unstructured environment. 
It has also been shown in several fields that multiple sources of information can complement each other,  overcoming the limitations of a single perceptual modality.
Hence, in this paper we introduce a data set of actions that includes both 
visual data (RGB-D video and 6DOF object pose estimation) and 
acoustic data. We also propose a method for recognizing and segmenting actions from 
continuous audio-visual data. The proposed method is employed for extensive evaluation
of the descriptive power of the two modalities, and we discuss how they 
can be used jointly to infer a coherent interpretation of the recorded action.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% INTRODUCTION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{}
\section{INTRODUCTION}
\label{sec:introduction}


There has been tremendous effort in the robotics community to develop robots able to operate autonomously in unstructured environments. Robots should be able to perceive the world correctly, detect objects, observe and interact with humans, perform activities and understand if the desired outcome has been achieved \cite{montesano08}.

Much effort has been spent on development of methods for visual perception and modeling of scenes. Many successful solutions have been proposed, considering various visual aspects such as object appearance, motion, human pose and affordances. %A challenge that these perception algorithms have in common is to integrate different kinds of information to formulate a consistent interpretation of the data. This approach strictly relates to the binding problem \cite{Treisman96} known in neuroscience. 
Nevertheless, the use of visual features has some limitations. Firstly, an action has to be performed within the field of view of the observer in order to be perceived. Secondly, object detection and tracking are very sensitive to occlusions while performing activities. Finally, there are meaningful states induced by an action that are hard to detect just relying on visual perception, it is, e.g., very difficult to detect if a person has turned on an oven.

One approach to tackle these limitation is to use additional sources of information, e.g., audio. While this field was in the past more focused on solving problems such as source localization and noise filtering, lately, the focus is more on exploiting sound to extract semantic knowledge and perform scene understanding \cite{Lyon10}. In many contexts, such information is very descriptive and can be helpful to understand a scene. Potentially it could compensate the limitations of visual perception (Fig.~\ref{fig:intro}). However, an important question arises: how can different modalities contribute to formulate a coherent interpretation of the world an agent is observing, in a manner similar to how humans process and integrate multiple modalities  \cite{Zmigrod13}?  Multimodality has up to now received relatively little focus in the robotics community, with some exceptions, e.g.,  \cite{TeoYDFA12}, and the task of audio-visual action recognition has not, to our knowledge, been addressed in robotics.



%\begin{figure}[t]
%	\vspace{2mm}
%\centerline{%
%	\subfigure[RGB-D video]{\includegraphics[width=0.48\linewidth]{images/intro/intro_rgb.png}}
%	%\includegraphics[width=0.48\linewidth]{images/intro/intro_disp.png}\label{fig:intro_rgbd}}}
%	%\centerline{%
%	\subfigure[Audio]{\includegraphics[width=0.48\linewidth]{images/intro/audio_intro.png}\label{fig:intro_audio}}}
%	%subfigure[Objects pose]{\includegraphics[width=0.48\linewidth]{images/intro/intro_track.png}\label{fig:intro_objects}}}
%\caption{An RGB-D video modality and an audio modality give complementary information about an observed human action. This is a motivation of why to use a recognition method that takes both modalities into account.}
%\vspace{-3mm}
%\label{fig:intro}
%\end{figure}

The main contribution of this paper is a {\em method for audio-visual recognition of human manipulation actions}. The visual features (Section \ref{sec:extractVideo}) are the relative 3D orientations and positions of objects involved in the activity, while the audio is represented in terms of MFCC features \cite{Hasan04} (Section \ref{sec:extractAudio}). Objects are tracked in 3D from an RGB-D video stream using the method in \cite{KarlTracker} (Section \ref{sec:tracker}). The audio and video cues are fused in an HMM framework (Section \ref{sec:model}). 

An additional contribution of the paper is an {\em RGB-D-audio dataset of humans involved in the activity of making milk and cereals} (Section \ref{sec:dataset}).  This composite activity consists of many sub-actions such as opening and closing boxes and bottles, and pouring milk and cereal. The recognition method described above was evaluated on this dataset (Section \ref{sec:experiments}). The dataset is annotated with manually extracted segmentation points between sub-actions, and 3D positions and orientations of objects, obtained as described above. It also features 3D models of all objects involved. We release source code for administration of and access to the dataset.


%% %%%%%%%%%%%%%%%%%%%%%%%%%%% RELATED WORK %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%\input{relatedWork}

%% %%%%%%%%%%%%%%%%%%%%%%%%%%% DATASET AND EVALUATION DESCRIPTION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\input{dataset}

%% %%%%%%%%%%%%%%%%%%%%%%%%%%% BENCHMARK %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{benchmark}

%% %%%%%%%%%%%%%%%%%%%%%%%%%%% CONCLUSION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{conclusion}

\bibliographystyle{unsrt}
\bibliography{ref}

\end{document}
