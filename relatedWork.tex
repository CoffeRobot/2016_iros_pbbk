\section{Related Work}
\label{sec:relatedwork}


Modeling and recognition of human activity from visual perception is an important problem tackled by the computer vision community, with applications in a wide variety of domains including health monitoring, visual surveillance, video search, human computer interaction and robot learning from demonstration. It has been shown \cite{desai10,gupta09a,kjellstrombook11,kjellstromCVIU11,laptev08,moore99,peursum05,veloso05,wu07} that it is advantageous to represent human activity both in terms of human motion and the objects involved in the activity. Several approaches are widely used to tackle this problem. \cite{Sapp10,AslamBB10a} estimate the human body pose  to recognize human activities. \cite{Bobick01} estimates human motion trajectories to infer activities. \cite{Laptev03} recognizes actions in videos by extracting low level spatio-temporal feature descriptors. 

However in the context of robot learning from demonstration it is meaningful to focus the attention on descriptive features that encodes how objects are being used by a human \cite{billard08},\cite{lang-toussaint10} to achieve a task. It is then possible that the robot can imitate the human and operate autonomously to accomplish given tasks. This has been confirmed in recent works  \cite{gall11,grabner11,rivlin95,stark-bowyer96,sutton-stark08,turek10} and it is supported by Gibson's affordance theory \cite{gibson79}, which states that humans and animals recognize objects in terms of function, in a task oriented manner.

\cite{aksoy10,luo11} present a global representation of activities in terms of spatial relations between objects. The recognition of activities is determined on a set of pre-defined symbols which describe the relationships between segmented coherent regions. In our previous work we have show that is also possible to describe object functionality in terms of how they are being handled by a human \cite{pieropan13}.
Moreover, \cite{grabner11} simulates humans performing activities using objects and cluster them into functional classes. In \cite{gupta11,moldovan12,gall11}, real humans are instead observed in interaction with a scene.

Visual perception supply part of the information to interpret the world. Much can be done from the auditory perspective. In \cite{SalviEtAlAffSpeech2012} learning object affordances was extended to auditory perception. However, the acoustic information was in the form of linguistic descriptions of the scene. In this work, on the contrary, we use the sound generated directly by the interaction between objects as a result of human actions. This is similar to what has been done in \cite{Stork12}. However what we propose here is to go one step further, we want to evaluate different sources of information in order to find the limitations and see how they can be jointly used to overcome such limitations. This is in the spirit with a similar work \cite{TeoYDFA12} where language has been exploited to perform action recognition. in order and tackle the same problem but at a higher level using information or features coming from different perceptual input. 

Most of the cited works tackles what is known in neuroscience as the binding problem \cite{Treisman96}, our brain formulate a coherent interpretation of the world from complex input merging multiple sources of information. Such a skill should be taken into account to develop autonomous robots.












